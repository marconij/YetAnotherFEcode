<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      -->
<title>All-at-once optimization for CP tensor decomposition</title>
<meta name="generator" content="MATLAB 24.2">
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
<meta name="DC.date" content="2025-09-23">
<meta name="DC.source" content="cp_opt_doc.m">
<style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:90%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:12px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:2.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }
.banner{ background-color:#15243c; text-align:center;}
.navigate {font-size:0.8em; padding:0px; line-height:100%; }

pre, code { font-size:14px; }
tt { font-size: 1.0em; font-weight:bold; background:#f7f7f7; padding-right:5px; padding-left:5px }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:20px 0px 0px; border-top:1px dotted #878787; font-size:0.9em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; padding:0px 20px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style>
</head>
<body>
<div class="banner">
<a href="index.html"><img src="Tensor-Toolbox-for-MATLAB-Banner.png"></a>
</div>
<div class="content">
<h1>All-at-once optimization for CP tensor decomposition</h1>
<!--introduction-->
<p>
<p class="navigate">
&#62;&#62; <a href="index.html">Tensor Toolbox</a>
&#62;&#62; <a href="cp.html">CP Decompositions</a>
&#62;&#62; <a href="cp_opt_doc.html">CP-OPT</a>
</p>
</p>
<p>We explain how to use <tt>cp_opt</tt> function which implements the <b>CP-OPT</b> method that fits the CP model using <i>direct</i> or <i>all-at-once</i> optimization. This is in contrast to the <tt>cp_als</tt> function which implements the <b>CP-ALS</b> method that fits the CP model using <i>alternating</i> optimization. The CP-OPT method is described in the following reference:</p>
<div>
<ul>
<li>E. Acar, D. M. Dunlavy and T. G. Kolda, A Scalable Optimization Approach for Fitting Canonical Tensor Decompositions, J. Chemometrics, 25(2):67-86, 2011, <a href="http://doi.org/10.1002/cem.1335">http://doi.org/10.1002/cem.1335</a>
</li>
</ul>
</div>
<p>This method works with any tensor that support the functions <tt>size</tt>, <tt>norm</tt>, and <tt>mttkrp</tt>. This includes not only <tt>tensor</tt> and <tt>sptensor</tt>, but also <tt>ktensor</tt>, <tt>ttensor</tt>, and <tt>sumtensor</tt>.</p>
<!--/introduction-->
<h2>Contents</h2>
<div>
<ul>
<li>
<a href="#1">Major overhaul in Tensor Toolbox Version 3.3, August 2022</a>
</li>
<li>
<a href="#2">Optimization methods</a>
</li>
<li>
<a href="#3">Optimization parameters</a>
</li>
<li>
<a href="#4">Simple example problem #1</a>
</li>
<li>
<a href="#5">Calling <tt>cp_opt</tt> method and evaluating its outputs</a>
</li>
<li>
<a href="#9">Specifying different optimization method</a>
</li>
<li>
<a href="#11">Calling <tt>cp_opt</tt> method with higher rank</a>
</li>
<li>
<a href="#14">Simple example problem #2 (nonnegative)</a>
</li>
<li>
<a href="#15">Call the <tt>cp_opt</tt> method with lower bound of zero</a>
</li>
<li>
<a href="#16">Reproducibility</a>
</li>
<li>
<a href="#17">Specifying different optimization method</a>
</li>
</ul>
</div>
<h2 id="1">Major overhaul in Tensor Toolbox Version 3.3, August 2022</h2>
<p>The code was completely overhauled in the Version 3.3 release of the Tensor Toolbox. The old code is available in <tt>cp_opt_legacy</tt> and documented <a href="cp_opt_legacy_doc.html">here</a>. The major differences are as follows:</p>
<div>
<ol>
<li>The function being optimized is now <img src="cp_opt_doc_eq16322453171028127602.png" alt="$\|X - M\|^2 / \|X\|^2$" style="width:106px;height:16px;"> where <i>X</i> is the data tensor and <i>M</i> is the model. Previously, the function being optimized was <img src="cp_opt_doc_eq17622441662993199522.png" alt="$\|X-M\|^2/2$" style="width:80px;height:16px;">. The new formulation is only different by a constant factor, but its advantage is that the convergence tests (e.g., norm of gradient) are less sensitive to the scale of the <i>X</i>.</li>
<li>We now support the MATLAB Optimization Toolbox methods, <tt>fminunc</tt> and <tt>fmincon</tt>, the later of which has support for bound contraints.</li>
<li>The input and output arguments are different. We've retained the <a href="cp_opt_legacy_doc.html">legacy version</a> for those that cannot easily change their workflow.</li>
</ol>
</div>
<h2 id="2">Optimization methods</h2>
<p>The <tt>cp_opt</tt> methods uses optimization methods from other packages; see <a href="opt_options_doc.html">Optimization Methods for Tensor Toolbox</a>. As of Version 3.3., we distribute the default method (<tt>'lbfgsb'</tt>) along with Tensor Toolbox.</p>
<p>Options for 'method':</p>
<div>
<ul>
<li>
<tt>'lbfgsb'</tt> (default) - Uses <a href="https://github.com/stephenbeckr/L-BFGS-B-C"><b>L-BFGS-B</b> by Stephen Becker</a>, which implements the bound-constrained, limited-memory BFGS method. This code is distributed along with the Tensor Toolbox and will be activiated on first use. Supports bound contraints.</li>
<li>
<tt>'lbfgs'</tt> - Uses <a href="https://software.sandia.gov/trac/poblano"><b>POBLANO</b> Version 1.2 by Evrim Acar, Daniel Dunlavy, and Tamara Kolda</a>, which implemented the limited-memory BFGS method. Does not support bound constraints, but it is pure MATLAB and may work if <tt>'lbfgsb'</tt> does not.</li>
<li>
<tt>'compact'</tt> - Uses <a href="https://github.com/johannesbrust/CR"><b>CR</b> by Johannes Brust</a>, which implements the limited-memory BFGS compact representation (CR) method. This code is distributed along with the Tensor Toolbox and will be activiated on first use. This is an alternative to Poblano.</li>
<li>
<tt>'fminunc'</tt> or <tt>'fmincon'</tt> - Routines provided by the <b>MATLAB Optimization Toolbox</b>. The latter supports bound constraints. (It also supports linear and nonlinear constraints, but we have not included an interface to those.)</li>
</ul>
</div>
<h2 id="3">Optimization parameters</h2>
<p>The full list of optimization parameters for each method are provide in <a href="opt_options_doc.html">Optimization Methods for Tensor Toolbox</a>. We list a few of the most relevant ones here.</p>
<div>
<ul>
<li>
<tt>'gtol'</tt> - The stopping condition for the norm of the gradient (or equivalent constrainted condition). Defaults to 1e-5.</li>
<li>
<tt>'lower'</tt> - Lower bounds, which can be a scalar (e.g., 0) or a vector. Defaults to <tt>-Inf</tt> (no lower bound).</li>
<li>
<tt>'printitn'</tt> - Frequency of printing. Normally, this specifies how often to print in terms of number of iteration. However, the MATLAB Optimization Toolbox method limit the options are 0 for none, 1 for every iteration, or &gt; 1 for just a final summary. These methods do not allow any more granularity than that.</li>
</ul>
</div>
<h2 id="4">Simple example problem #1</h2>
<p>Create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise.</p>
<pre class="codeinput">rng(1); <span class="comment">% Reproducibility</span>
R1 = 5;
problem1 = create_problem(<span class="string">'Size'</span>, [50 40 30], <span class="string">'Num_Factors'</span>, R, <span class="string">'Noise'</span>, 0.10);
X1 = problem1.Data;
M1_true = problem1.Soln;

<span class="comment">% Create initial guess using 'nvecs'</span>
M1_init = create_guess(<span class="string">'Data'</span>, X1, <span class="string">'Num_Factors'</span>, R1, <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre>
<h2 id="5">Calling <tt>cp_opt</tt> method and evaluating its outputs</h2>
<p>Here is an example call to the cp_opt method. By default, each iteration prints the least squares fit function value (being minimized) and the norm of the gradient.</p>
<pre class="codeinput">rng(<span class="string">'default'</span>); <span class="comment">% Reproducibility</span>
[M1,~,info1] = cp_opt(X1, R1, <span class="string">'init'</span>, M1_init, <span class="string">'printitn'</span>, 25);
</pre>
<pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 5
Lower bound: -Inf, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 5.472721e-02, ||grad||_infty = 5.95e-04
Iter    49, f(x) = 9.809441e-03, ||grad||_infty = 5.72e-06
End Main Loop

Final f: 9.8094e-03
Setup time: 0.00043 seconds
Optimization time: 0.095 seconds
Iterations: 49
Total iterations: 116
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.
</pre>
<p>It's important to check the output of the optimization method. In particular, it's worthwhile to check the exit message. The message <tt>CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH</tt> means that it has converged because the function value stopped improving. The message <tt>CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL</tt> means that the gradient is sufficiently small.</p>
<pre class="codeinput">exitmsg = info1.optout.exit_condition;
display(exitmsg);
</pre>
<pre class="codeoutput">
exitmsg =

    'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.'

</pre>
<p>The objective function here is different than for CP-ALS. That uses the fit, which is <img src="cp_opt_doc_eq07029206319979379429.png" alt="$1 - (\|X-M\|/\|X\|)$" style="width:131px;height:15px;">. In other words, it is the percentage of the data that is explained by the model. Because we have 10% noise, we do not expect the fit to be about 90%.</p>
<pre class="codeinput">fit1 = 1 - sqrt(info1.f);
display(fit1);
</pre>
<pre class="codeoutput">
fit1 =

    0.9010

</pre>
<p>We can "score" the similarity of the model computed by CP and compare that with the truth. The <tt>score</tt> function on ktensor's gives a score in [0,1] with 1 indicating a perfect match. Because we have noise, we do not expect the fit to be perfect. See <a href="matlab:doc('ktensor/score')">doc score</a> for more details.</p>
<pre class="codeinput">scr1 = score(M1,M1_true);
display(scr1);
</pre>
<pre class="codeoutput">
scr1 =

    0.9985

</pre>
<h2 id="9">Specifying different optimization method</h2>
<pre class="codeinput">rng(<span class="string">'default'</span>); <span class="comment">% Reproducibility</span>
[M1b,~,info1b] = cp_opt(X1, R1, <span class="string">'init'</span>, M1_init, <span class="string">'printitn'</span>, 25, <span class="string">'method'</span>, <span class="string">'compact'</span>);
</pre>
<pre class="codeoutput">
CP-OPT CP Direct Optimzation
CR solver (limited-memory) (https://github.com/johannesbrust/CR)
Size: 50 x 40 x 30, Rank: 5
Parameters: m=5, gtol=1e-05, maxiters = 1000 subiters = 500

Begin Main Loop
Compact Representation (ALG0)  

Line-search: wolfeG          
V-strategy:  s          
         n=  600          
--------------------------------
k    	 nf      	 fk         	 ||gk||         	 step        	 iexit       
0 	 1 	 9.87861e-01      	 2.00132e-02       	 1.000e+00     	 0
25 	 34 	 5.52331e-02      	 1.71379e-03       	 1.000e+00     	 1 	         
50 	 67 	 9.81030e-03      	 1.64187e-04       	 1.000e+00     	 1 	         
End Main Loop

Final f: 9.8094e-03
Setup time: 0.00029 seconds
Optimization time: 0.066 seconds
Iterations: 58
Total iterations: 76
Exit condition: Convergence: Norm tolerance
</pre>
<p>Check exit condition</p>
<pre class="codeinput">exitmsg = info1b.optout.exit_condition;
display(exitmsg);
<span class="comment">% Fit</span>
fit1b = 1 - sqrt(info1b.f);
display(fit1b);
<span class="comment">% Score</span>
scr1b = score(M1b,M1_true);
display(scr1b);
</pre>
<pre class="codeoutput">
exitmsg =

    'Convergence: Norm tolerance'


fit1b =

    0.9010


scr1b =

    0.9986

</pre>
<h2 id="11">Calling <tt>cp_opt</tt> method with higher rank</h2>
<p>Re-using the same example as before, consider the case where we don't know R in advance. We might guess too high. Here we show a case where we guess R+1 factors rather than R.</p>
<pre class="codeinput">
<span class="comment">% Generate initial guess of the correct size</span>
rng(2);
M1_plus_init = create_guess(<span class="string">'Data'</span>, X1, <span class="string">'Num_Factors'</span>, R1+1, <span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'nvecs'</span>);
</pre>
<pre class="codeinput">
<span class="comment">% Run the algorithm</span>
rng(<span class="string">'default'</span>); <span class="comment">% Reproducibility</span>
[M1c,~,output1c] = cp_opt(X1, R1+1, <span class="string">'init'</span>, M1_plus_init,<span class="string">'printitn'</span>,25);
</pre>
<pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 6
Lower bound: -Inf, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 5.462523e-02, ||grad||_infty = 6.90e-04
Iter    50, f(x) = 9.788408e-03, ||grad||_infty = 1.11e-05
Iter    65, f(x) = 9.775966e-03, ||grad||_infty = 7.19e-06
End Main Loop

Final f: 9.7760e-03
Setup time: 0.00021 seconds
Optimization time: 0.057 seconds
Iterations: 65
Total iterations: 146
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.
</pre>
<pre class="codeinput">
<span class="comment">% Check exit condition</span>
exitmsg1c = output1c.optout.exit_condition;
display(exitmsg1c);
<span class="comment">% Fit</span>
fit1c = 1 - sqrt(output1c.f);
display(fit1c);
<span class="comment">% Check the answer (1 is perfect)</span>
scr1c = score(M1c, M1_true);
display(scr1c);
</pre>
<pre class="codeoutput">
exitmsg1c =

    'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.'


fit1c =

    0.9011


scr1c =

    0.9985

</pre>
<h2 id="14">Simple example problem #2 (nonnegative)</h2>
<p>We can employ lower bounds to get a nonnegative factorization. First, we create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise. We select nonnegative factor matrices and lambdas. The create_problem doesn't really know how to add noise without going negative, so we <i>hack</i> it to make the observed tensor be nonzero.</p>
<pre class="codeinput">R2 = 5;
rng(3);
problem2 = create_problem(<span class="string">'Size'</span>, [50 40 30], <span class="string">'Num_Factors'</span>, R2, <span class="string">'Noise'</span>, 0.10,<span class="keyword">...</span>
    <span class="string">'Factor_Generator'</span>, <span class="string">'rand'</span>, <span class="string">'Lambda_Generator'</span>, <span class="string">'rand'</span>);
X2 = problem2.Data .* (problem2.Data &gt; 0); <span class="comment">% Force it to be nonnegative</span>
M2_true = problem2.Soln;
</pre>
<h2 id="15">Call the <tt>cp_opt</tt> method with lower bound of zero</h2>
<p>Here we specify a lower bound of zero with the last two arguments.</p>
<pre class="codeinput">rng(<span class="string">'default'</span>); <span class="comment">% Reproducibility</span>
[M2,M20,info2] = cp_opt(X2, R2, <span class="string">'init'</span>, <span class="string">'rand'</span>,<span class="string">'lower'</span>,0,<span class="string">'printitn'</span>,25);

<span class="comment">% Check the output</span>
exitmsg2 = info2.optout.exit_condition;
display(exitmsg2);

<span class="comment">% Check the fit</span>
fit2 = 1 - sqrt(info2.f);
display(fit2);

<span class="comment">% Evaluate the output</span>
scr2 = score(M2,M2_true);
display(scr2);
</pre>
<pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 5
Lower bound: 0, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 1.607417e-02, ||grad||_infty = 1.85e-03
Iter    50, f(x) = 1.023857e-02, ||grad||_infty = 5.09e-04
Iter    75, f(x) = 9.791157e-03, ||grad||_infty = 1.44e-04
Iter   100, f(x) = 9.775862e-03, ||grad||_infty = 1.04e-04
Iter   108, f(x) = 9.775178e-03, ||grad||_infty = 1.10e-04
End Main Loop

Final f: 9.7752e-03
Setup time: 0.00038 seconds
Optimization time: 0.078 seconds
Iterations: 108
Total iterations: 225
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.

exitmsg2 =

    'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.'


fit2 =

    0.9011


scr2 =

    0.9929

</pre>
<h2 id="16">Reproducibility</h2>
<p>The parameters of a run are saved, so that a run can be reproduced exactly as follows.</p>
<pre class="codeinput">rng(6); <span class="comment">% Different random state</span>
cp_opt(X2,R2,info2.params);
</pre>
<pre class="codeoutput">
CP-OPT CP Direct Optimzation
L-BFGS-B Optimization (https://github.com/stephenbeckr/L-BFGS-B-C)
Size: 50 x 40 x 30, Rank: 5
Lower bound: 0, Upper bound: Inf
Parameters: m=5, ftol=1e-10, gtol=1e-05, maxiters = 1000, subiters = 10

Begin Main Loop
Iter    25, f(x) = 1.607417e-02, ||grad||_infty = 1.85e-03
Iter    50, f(x) = 1.023857e-02, ||grad||_infty = 5.09e-04
Iter    75, f(x) = 9.791157e-03, ||grad||_infty = 1.44e-04
Iter   100, f(x) = 9.775862e-03, ||grad||_infty = 1.04e-04
Iter   108, f(x) = 9.775178e-03, ||grad||_infty = 1.10e-04
End Main Loop

Final f: 9.7752e-03
Setup time: 0.00035 seconds
Optimization time: 0.1 seconds
Iterations: 108
Total iterations: 225
Exit condition: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL.
</pre>
<h2 id="17">Specifying different optimization method</h2>
<pre class="codeinput">rng(<span class="string">'default'</span>); <span class="comment">% Reproducibility</span>
[M2b,~,info2b] = cp_opt(X2, R2, <span class="string">'init'</span>, M20, <span class="string">'printitn'</span>, 25, <span class="keyword">...</span>
    <span class="string">'method'</span>, <span class="string">'compact'</span>, <span class="string">'lower'</span>, 0);
</pre>
<pre class="codeoutput">
CP-OPT CP Direct Optimzation
CR solver (limited-memory) (https://github.com/johannesbrust/CR)
Size: 50 x 40 x 30, Rank: 5
Parameters: m=5, gtol=1e-05, maxiters = 1000 subiters = 500

Begin Main Loop
Compact Representation (ALG0)  

Line-search: wolfeG          
V-strategy:  s          
         n=  600          
--------------------------------
k    	 nf      	 fk         	 ||gk||         	 step        	 iexit       
0 	 1 	 3.86664e+00      	 2.40555e+00       	 1.000e+00     	 0
25 	 29 	 1.57093e-02      	 7.62420e-03       	 1.000e+00     	 1 	         
50 	 54 	 1.03890e-02      	 2.97993e-03       	 1.000e+00     	 1 	         
75 	 81 	 9.81333e-03      	 7.01183e-04       	 1.000e+00     	 1 	         
100 	 109 	 9.77713e-03      	 2.66092e-04       	 1.000e+00     	 1 	         
125 	 136 	 9.77473e-03      	 2.67276e-05       	 1.000e+00     	 1 	         
End Main Loop

Final f: 9.7747e-03
Setup time: 0.00016 seconds
Optimization time: 0.16 seconds
Iterations: 137
Total iterations: 149
Exit condition: Convergence: Norm tolerance
</pre>
<pre class="codeinput">
<span class="comment">% Check exit condition</span>
exitmsg2b = info2b.optout.exit_condition;
display(exitmsg2b);
<span class="comment">% Fit</span>
fit2b = 1 - sqrt(info2b.f);
display(fit2b);
<span class="comment">% Score</span>
scr2b = score(M2b,M2_true);
display(scr2b);
</pre>
<pre class="codeoutput">
exitmsg2b =

    'Convergence: Norm tolerance'


fit2b =

    0.9011


scr2b =

    0.9935

</pre>
<p class="footer">Tensor Toolbox for MATLAB: <a href="index.html">www.tensortoolbox.org</a>.</p>
</div>
<!--
##### SOURCE BEGIN #####
%% All-at-once optimization for CP tensor decomposition
%
% <html>
% <p class="navigate">
% &#62;&#62; <a href="index.html">Tensor Toolbox</a> 
% &#62;&#62; <a href="cp.html">CP Decompositions</a> 
% &#62;&#62; <a href="cp_opt_doc.html">CP-OPT</a>
% </p>
% </html>
%
% We explain how to use |cp_opt| function which implements the *CP-OPT*
% method that fits the CP model using _direct_ or _all-at-once_
% optimization. This is in contrast to the |cp_als| function which
% implements the *CP-ALS* method that fits the CP model using _alternating_ 
% optimization. The CP-OPT method is described in the
% following reference: 
%
% * E. Acar, D. M. Dunlavy and T. G. Kolda, A Scalable
% Optimization Approach for Fitting Canonical Tensor Decompositions,
% J. Chemometrics, 25(2):67-86, 2011,
% <http://doi.org/10.1002/cem.1335>
%
% This method works with any tensor that support the functions |size|,
% |norm|, and |mttkrp|. This includes not only |tensor| and |sptensor|, but
% also |ktensor|, |ttensor|, and |sumtensor|.

%% Major overhaul in Tensor Toolbox Version 3.3, August 2022
% The code was completely overhauled in the Version 3.3 release of the
% Tensor Toolbox. The old code is available in |cp_opt_legacy| and
% documented <cp_opt_legacy_doc.html here>. The major 
% differences are as follows:
% 
% # The function being optimized is now $\|X - M\|^2 / \|X\|^2$ where _X_
% is the data tensor and _M_ is the model. Previously, the function being
% optimized was $\|X-M\|^2/2$. The new formulation is only different by a
% constant factor, but its advantage is that the convergence tests (e.g.,
% norm of gradient) are less sensitive to the scale of the _X_. 
% # We now support the MATLAB Optimization Toolbox methods, |fminunc| and
% |fmincon|, the later of which has support for bound contraints.
% # The input and output arguments are different. 
% We've retained the <cp_opt_legacy_doc.html legacy version> for those that
% cannot easily change their workflow. 
% 

%% Optimization methods
% The |cp_opt| methods uses optimization methods from other packages; 
% see <opt_options_doc.html Optimization Methods for Tensor Toolbox>. 
% As of Version 3.3., we distribute the default method (|'lbfgsb'|)
% along with Tensor Toolbox.
%
% Options for 'method':
%
% * |'lbfgsb'| (default) - Uses 
% <https://github.com/stephenbeckr/L-BFGS-B-C *L-BFGS-B* by Stephen Becker>, 
% which implements the bound-constrained, limited-memory BFGS method. This
% code is distributed along with the Tensor Toolbox and will be activiated
% on first use. Supports bound contraints.
% * |'lbfgs'| - Uses 
% <https://software.sandia.gov/trac/poblano *POBLANO* Version 1.2 by
% Evrim Acar, Daniel Dunlavy, and Tamara Kolda>, which implemented the
% limited-memory BFGS method. Does not support bound constraints, but it is
% pure MATLAB and may work if |'lbfgsb'| does not.
% * |'compact'| - Uses 
% <https://github.com/johannesbrust/CR *CR* by Johannes Brust>, which
% implements the limited-memory BFGS compact representation (CR) method. This
% code is distributed along with the Tensor Toolbox and will be activiated
% on first use. This is an alternative to Poblano.
% * |'fminunc'| or |'fmincon'| - Routines provided by the *MATLAB Optimization
% Toolbox*. The latter supports bound constraints. (It also supports linear and nonlinear constraints, but we
% have not included an interface to those.)


%% Optimization parameters
% The full list of optimization parameters for each method are provide in 
% <opt_options_doc.html Optimization Methods for Tensor Toolbox>.
% We list a few of the most relevant ones here.
%
% * |'gtol'| - The stopping condition for the norm of the gradient (or
% equivalent constrainted condition). Defaults to 1e-5.
% * |'lower'| - Lower bounds, which can be a scalar (e.g., 0) or a vector.
% Defaults to |-Inf| (no lower bound).
% * |'printitn'| - Frequency of printing. Normally, this specifies how 
% often to print in terms of number of iteration. 
% However, the MATLAB Optimization Toolbox method limit the options are 0
% for none, 1 for every iteration, or > 1 for just a final summary. These
% methods do not allow any more granularity than that.

%% Simple example problem #1
% Create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise.
rng(1); % Reproducibility
R1 = 5;
problem1 = create_problem('Size', [50 40 30], 'Num_Factors', R, 'Noise', 0.10);
X1 = problem1.Data;
M1_true = problem1.Soln;

% Create initial guess using 'nvecs'
M1_init = create_guess('Data', X1, 'Num_Factors', R1, 'Factor_Generator', 'nvecs');

%% Calling |cp_opt| method and evaluating its outputs
% Here is an example call to the cp_opt method. By default, each iteration
% prints the least squares fit function value (being minimized) and the
% norm of the gradient. 

rng('default'); % Reproducibility
[M1,~,info1] = cp_opt(X1, R1, 'init', M1_init, 'printitn', 25);

%% 
% It's important to check the output of the optimization method. In
% particular, it's worthwhile to check the exit message. 
% The message |CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH| means that
% it has converged because the function value stopped improving.
% The message |CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL| means that
% the gradient is sufficiently small.
exitmsg = info1.optout.exit_condition;
display(exitmsg);

%% 
% The objective function here is different than for CP-ALS. That uses the
% fit, which is $1 - (\|X-M\|/\|X\|)$. In other words, it is the percentage
% of the data that is explained by the model. 
% Because we have 10% noise, we do not expect the fit to be about 90%.
fit1 = 1 - sqrt(info1.f);
display(fit1);

%% 
% We can "score" the similarity of the model computed by CP and compare
% that with the truth. The |score| function on ktensor's gives a score in
% [0,1]  with 1 indicating a perfect match. Because we have noise, we do
% not expect the fit to be perfect. See <matlab:doc('ktensor/score') doc
% score> for more details.
scr1 = score(M1,M1_true);
display(scr1);

%% Specifying different optimization method
rng('default'); % Reproducibility
[M1b,~,info1b] = cp_opt(X1, R1, 'init', M1_init, 'printitn', 25, 'method', 'compact');

%%
% Check exit condition
exitmsg = info1b.optout.exit_condition;
display(exitmsg);
% Fit
fit1b = 1 - sqrt(info1b.f);
display(fit1b);
% Score
scr1b = score(M1b,M1_true);
display(scr1b);

%% Calling |cp_opt| method with higher rank
% Re-using the same example as before, consider the case where we don't
% know R in advance. We might guess too high. Here we show a case where we
% guess R+1 factors rather than R. 

% Generate initial guess of the correct size
rng(2);
M1_plus_init = create_guess('Data', X1, 'Num_Factors', R1+1, ...
    'Factor_Generator', 'nvecs');

%%

% Run the algorithm
rng('default'); % Reproducibility
[M1c,~,output1c] = cp_opt(X1, R1+1, 'init', M1_plus_init,'printitn',25);

%%

% Check exit condition
exitmsg1c = output1c.optout.exit_condition;
display(exitmsg1c);
% Fit
fit1c = 1 - sqrt(output1c.f);
display(fit1c);
% Check the answer (1 is perfect)
scr1c = score(M1c, M1_true);
display(scr1c);

%% Simple example problem #2 (nonnegative)
% We can employ lower bounds to get a nonnegative factorization.
% First, we create an example 50 x 40 x 30 tensor with rank 5 and add 10% noise. We
% select nonnegative factor matrices and lambdas. The
% create_problem doesn't really know how to add noise without going
% negative, so we _hack_ it to make the observed tensor be nonzero.
R2 = 5;
rng(3);
problem2 = create_problem('Size', [50 40 30], 'Num_Factors', R2, 'Noise', 0.10,...
    'Factor_Generator', 'rand', 'Lambda_Generator', 'rand');
X2 = problem2.Data .* (problem2.Data > 0); % Force it to be nonnegative
M2_true = problem2.Soln;

%% Call the |cp_opt| method with lower bound of zero
% Here we specify a lower bound of zero with the last two arguments.
rng('default'); % Reproducibility
[M2,M20,info2] = cp_opt(X2, R2, 'init', 'rand','lower',0,'printitn',25);

% Check the output
exitmsg2 = info2.optout.exit_condition;
display(exitmsg2);

% Check the fit
fit2 = 1 - sqrt(info2.f);
display(fit2);

% Evaluate the output
scr2 = score(M2,M2_true);
display(scr2);

%% Reproducibility
% The parameters of a run are saved, so that a run can be reproduced
% exactly as follows.  
rng(6); % Different random state
cp_opt(X2,R2,info2.params);

%% Specifying different optimization method
rng('default'); % Reproducibility
[M2b,~,info2b] = cp_opt(X2, R2, 'init', M20, 'printitn', 25, ...
    'method', 'compact', 'lower', 0);

%%

% Check exit condition
exitmsg2b = info2b.optout.exit_condition;
display(exitmsg2b);
% Fit
fit2b = 1 - sqrt(info2b.f);
display(fit2b);
% Score
scr2b = score(M2b,M2_true);
display(scr2b);
##### SOURCE END #####
-->
</body>
</html>
